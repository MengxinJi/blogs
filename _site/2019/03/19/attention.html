<!DOCTYPE html>

<html>

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>
    Attention - Helen&#39;s Blog
    
  </title>

  <meta name="description" content="Attention is a mechanism for improving the effects of the RNN (LSTM or GRU) based Encoder + Decoder model, commonly referred to as the Attention Mechanism. A...">

  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="/blogs/assets/vendor/bootstrap/css/bootstrap.min.css">

  <link rel="stylesheet" href="/blogs/assets/vendor/fontawesome-free/css/all.min.css">

  <link rel="stylesheet" href="/blogs/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/blogs/2019/03/19/attention.html">
  <link rel="alternate" type="application/rss+xml" title="Helen&#39;s Blog" href="/blogs/feed.xml">

</head>


<body>

  <!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="/blogs/">Helen&#39;s Blog</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      Menu
      <i class="fa fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a class="nav-link" href="/blogs/">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blogs/about">About</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blogs/posts">Posts</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/blogs/contact">Contact</a>
        </li>
      </ul>
    </div>
  </div>
</nav>


  <!-- Page Header -->

  <header class="masthead">
    
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="post-heading">
            <h1>Attention</h1>
            
            <span class="meta">Posted by
              <a href="#">Helen Ji</a>
              on March 19, 2019 &middot; <span class="reading-time" title="Estimated read time">
  
   5 mins  read </span>

            </span>
          </div>
        </div>
      </div>
    </div>
  </header>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">

        <p>Attention is a mechanism for improving the effects of the RNN (LSTM or GRU) based Encoder + Decoder model, commonly referred to as the Attention Mechanism. Attention Mechanism is very popular at present, widely used in many fields such as machine translation, speech recognition, image caption, etc. It is so popular because Attention gives the model the ability to distinguish between regions, for example, in machine translation, speech recognition applications, each word in a sentence is given different weights, making the learning of the neural network model more flexible, and Attention itself can be used as an alignment relationship to interpret translation between input/output sentences. Hence, attention in deep learning can be viewed as a vector of importance weights: we estimate or classify using the attention vector how strongly it is correlated with other elements and take the sum of their values weighted by the attention vector as the approximation of the target.</p>

<h3 id="two-problems-attention-mechanism-solves">Two problems Attention Mechanism Solves</h3>

<p><a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Seq2seq</a> introduces a model based on an Encoder and a Decoder to build a neural network based end-to-end machine translation model, in which, Encoder encodes the input X in a fixed-length hidden vector Z and Decoder decodes the target output Y based on the hidden vector Z. This is a classic seq2seq model, but there are two obvious problems:</p>

<ul>
  <li>
    <p>Compress all information of input X to a fixed length of hidden vector Z while ignoring the true length of X, will result in the sharp decline of performance especially when the length of input X is longer than initial lengths from training dataset.</p>
  </li>
  <li>
    <p>It is unreasonable to encode the input X into a fixed length and give the same weight to each word in the sentence. For example, in machine translation, between input sentence and the output sentence, one or several words usually correspond to one or several words. Hence, each word entered given the same weight does not provide discrimination.</p>
  </li>
</ul>

<p>Hence, this fixed-length context vector design will lead to a critical and apparent disadvantage that is incapability of remembering long sentences. Often it has forgotten the first part once it completes processing the whole input. Now, itâ€™s time to introduce <a href="https://arxiv.org/pdf/1409.0473.pdf">Attention</a> that resolves the above problems.</p>

<h2 id="attention-mechanism">Attention Mechanism</h2>

<p>The attention mechanism was born to help memorize long source sentences in neural machine translation. The secret sauce invented by attention is to create shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.</p>

<p>In the paper, they construct the following architecture with three pieces of information:</p>

<ul>
  <li>encoder hidden states;</li>
  <li>decoder hidden states;</li>
  <li>alignment between source and target;</li>
</ul>

<p><img src="https://mengxinji.github.io/Blog/images/encoder-decoder-attention.png" alt="Attention Mcehanism" style="float:right; padding:16px" /></p>

<p>In the model, they define a conditional probability:</p>

<p><img src="https://mengxinji.github.io/Blog/images/attention/prob.jpg" alt="conditional probability" style="float:bottom; padding:16px" /></p>

<p>where, si is the hidden state from RNN in decoder:</p>

<p><img src="https://mengxinji.github.io/Blog/images/attention/si.jpg" alt="hidden state" style="float:bottom; padding:16px" /></p>

<p>Here ci is a weighted value, defined as:</p>

<p><img src="https://mengxinji.github.io/Blog/images/attention/ci.jpg" alt="weighted value" style="float:bottom; padding:16px" /></p>

<p>where i is the ith word from encoder, hj is the jth hidden vector from encoder, aij is the weighted value between jth word from encoder and ith word from decoder, indicating the effect from jth word from source to ith word in target. aij is calculated as:</p>

<p><img src="https://mengxinji.github.io/Blog/images/attention/aij.jpg" alt="conditional probability" style="float:bottom; padding:16px" /></p>

<p>Here, aij is the softmax output, with the summation equal 1. eij represents the alignment, to estimate the alignment effect from jth word from encoder to ith word from decoder. There are different alignment score functions, and the basic one is defined as follows, known as dot product, i.e., dot product between hidden state from target output and hidden state from source output.</p>

<p><img src="https://mengxinji.github.io/Blog/images/attention/score.jpg" alt="conditional probability" style="float:bottom; padding:16px" /></p>

<h2 id="different-types-of-attention-mechanism">Different Types of Attention Mechanism</h2>

<h3 id="self-attention">Self-Attention</h3>

<p>Self Attention is very different from the traditional Attention mechanism: the traditional Attention is based on the source and target hidden states to calculate Attention, and the result is the dependence between each word at the source and each word at the target. However, for self Attention, it is performed on the source and target sides respectively, and only use the Self Attention associated with the source input or the target input itself to capture the dependency between the source and the target itself. Then it adds the self Attention from the source to the self Attention from the target to capture the relationship between the source and the target. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.</p>

<h3 id="soft-attention-and-hard-attention">Soft Attention and Hard Attention</h3>

<h3 id="global-attention-and-local-attention">Global Attention and Local Attention</h3>



        <hr>

        <div class="clearfix">

          
          <a class="btn btn-primary float-left" href="/blogs/2019/03/13/RNN-and-LSTM.html" data-toggle="tooltip" data-placement="top" title="Markov Decision Process">&larr; Previous<span class="d-none d-md-inline">
              Post</span></a>
          
          
          <a class="btn btn-primary float-right" href="/blogs/2019/03/21/transformer.html" data-toggle="tooltip" data-placement="top" title="Transformer">Next<span class="d-none d-md-inline">
              Post</span> &rarr;</a>
          

        </div>

      </div>
    </div>
  </div>


  <!-- Footer -->

<hr>

<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        <ul class="list-inline text-center">
          
          <li class="list-inline-item">
            <a href="mailto:mengxin.ji1992@gmail.com">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="far fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://twitter.com/Helen_econ">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://www.facebook.com/mumujimx">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://www.linkedin.com/in/mengxinji">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li class="list-inline-item">
            <a href="https://github.com/MengxinJi">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fab fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="copyright text-muted">Copyright &copy; Helen Ji 2019</p>
      </div>
    </div>
  </div>
</footer>


  <script src="/blogs/assets/vendor/jquery/jquery.min.js"></script>
<script src="/blogs/assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="/blogs/assets/vendor/startbootstrap-clean-blog/js/clean-blog.min.js"></script>

<script src="/blogs/assets/scripts.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX-X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-XXXXXXXXX-X');
</script>



</body>

</html>
